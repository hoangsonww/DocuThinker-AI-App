stages:
  - pre-check
  - build
  - test
  - security
  - package
  - deploy-dev
  - deploy-staging
  - deploy-production
  - post-deploy
  - cleanup

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  REGISTRY: "registry.gitlab.com/$CI_PROJECT_PATH"
  KUBECONFIG: /tmp/kubeconfig
  HELM_VERSION: "3.13.0"
  TERRAFORM_VERSION: "1.5.7"
  TRIVY_VERSION: "0.48.0"
  SONAR_VERSION: "5.0.1"

default:
  image: node:18-alpine
  cache:
    key:
      files:
        - package-lock.json
        - frontend/package-lock.json
        - backend/package-lock.json
    paths:
      - node_modules/
      - frontend/node_modules/
      - backend/node_modules/
  retry:
    max: 2
    when:
      - runner_system_failure
      - stuck_or_timeout_failure

# ============== PRE-CHECK STAGE ==============

code-quality:
  stage: pre-check
  script:
    - npm ci
    - npm run lint
  only:
    - merge_requests
    - master
  allow_failure: false

dependency-check:
  stage: pre-check
  image: node:18
  script:
    - npm audit --production --audit-level=moderate
    - cd frontend && npm audit --production --audit-level=moderate
    - cd ../backend && npm audit --production --audit-level=moderate
  allow_failure: true
  only:
    - merge_requests
    - master

# ============== BUILD STAGE ==============

build-frontend:
  stage: build
  script:
    - cd frontend
    - npm ci
    - npm run build
  artifacts:
    paths:
      - frontend/build/
    expire_in: 1 hour
  only:
    - merge_requests
    - master
    - develop

build-backend:
  stage: build
  script:
    - cd backend
    - npm ci
    - echo "Backend build completed"
  artifacts:
    paths:
      - backend/
    expire_in: 1 hour
  only:
    - merge_requests
    - master
    - develop

build-ai-ml:
  stage: build
  image: python:3.10-slim
  script:
    - cd ai_ml
    - pip install -r requirements.txt
    - python -m compileall .
  artifacts:
    paths:
      - ai_ml/
    expire_in: 1 hour
  only:
    - merge_requests
    - master
    - develop

# ============== TEST STAGE ==============

test-frontend:
  stage: test
  dependencies:
    - build-frontend
  script:
    - cd frontend
    - npm ci
    - npm run test -- --coverage --watchAll=false
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: frontend/coverage/cobertura-coverage.xml
    paths:
      - frontend/coverage/
    expire_in: 30 days
  only:
    - merge_requests
    - master
    - develop

test-backend:
  stage: test
  dependencies:
    - build-backend
  script:
    - cd backend
    - npm ci
    - npm run test -- --coverage
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: backend/coverage/cobertura-coverage.xml
    paths:
      - backend/coverage/
    expire_in: 30 days
  only:
    - merge_requests
    - master
    - develop

test-ai-ml:
  stage: test
  image: python:3.10-slim
  dependencies:
    - build-ai-ml
  script:
    - cd ai_ml
    - pip install -r requirements.txt pytest pytest-cov
    - pytest --cov=. --cov-report=xml --cov-report=html
  coverage: '/TOTAL.*\s+(\d+%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: ai_ml/coverage.xml
    paths:
      - ai_ml/htmlcov/
    expire_in: 30 days
  only:
    - merge_requests
    - master
    - develop

# ============== SECURITY STAGE ==============

trivy-scan-frontend:
  stage: security
  image: aquasec/trivy:latest
  dependencies:
    - build-frontend
  script:
    - trivy fs --security-checks vuln,config frontend/
    - trivy fs --format json --output trivy-frontend-report.json frontend/
  artifacts:
    reports:
      container_scanning: trivy-frontend-report.json
    paths:
      - trivy-frontend-report.json
    expire_in: 7 days
  allow_failure: true
  only:
    - merge_requests
    - master

trivy-scan-backend:
  stage: security
  image: aquasec/trivy:latest
  dependencies:
    - build-backend
  script:
    - trivy fs --security-checks vuln,config backend/
    - trivy fs --format json --output trivy-backend-report.json backend/
  artifacts:
    reports:
      container_scanning: trivy-backend-report.json
    paths:
      - trivy-backend-report.json
    expire_in: 7 days
  allow_failure: true
  only:
    - merge_requests
    - master

sonarqube-scan:
  stage: security
  image: sonarsource/sonar-scanner-cli:latest
  variables:
    SONAR_USER_HOME: "${CI_PROJECT_DIR}/.sonar"
    GIT_DEPTH: "0"
  cache:
    key: "${CI_JOB_NAME}"
    paths:
      - .sonar/cache
  script:
    - sonar-scanner
      -Dsonar.projectKey=docuthinker
      -Dsonar.sources=.
      -Dsonar.host.url=$SONAR_HOST_URL
      -Dsonar.login=$SONAR_TOKEN
      -Dsonar.javascript.lcov.reportPaths=frontend/coverage/lcov.info,backend/coverage/lcov.info
  allow_failure: true
  only:
    - merge_requests
    - master

# ============== PACKAGE STAGE ==============

docker-build-frontend:
  stage: package
  image: docker:24-dind
  services:
    - docker:24-dind
  dependencies:
    - build-frontend
  before_script:
    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY
  script:
    - docker build -t $REGISTRY/frontend:$CI_COMMIT_SHA -t $REGISTRY/frontend:latest frontend/
    - docker push $REGISTRY/frontend:$CI_COMMIT_SHA
    - docker push $REGISTRY/frontend:latest
  only:
    - master
    - develop

docker-build-backend:
  stage: package
  image: docker:24-dind
  services:
    - docker:24-dind
  dependencies:
    - build-backend
  before_script:
    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY
  script:
    - docker build -t $REGISTRY/backend:$CI_COMMIT_SHA -t $REGISTRY/backend:latest backend/
    - docker push $REGISTRY/backend:$CI_COMMIT_SHA
    - docker push $REGISTRY/backend:latest
  only:
    - master
    - develop

# ============== DEPLOYMENT STAGES ==============

deploy-dev:
  stage: deploy-dev
  image: alpine/k8s:1.28.0
  environment:
    name: development
    url: https://dev.docuthinker.com
  before_script:
    - echo "$KUBE_CONFIG_DEV" | base64 -d > $KUBECONFIG
    - kubectl config use-context dev-cluster
  script:
    - kubectl set image deployment/frontend frontend=$REGISTRY/frontend:$CI_COMMIT_SHA -n docuthinker-dev
    - kubectl set image deployment/backend backend=$REGISTRY/backend:$CI_COMMIT_SHA -n docuthinker-dev
    - kubectl rollout status deployment/frontend -n docuthinker-dev --timeout=5m
    - kubectl rollout status deployment/backend -n docuthinker-dev --timeout=5m
  only:
    - develop
  when: on_success

deploy-staging:
  stage: deploy-staging
  image: alpine/k8s:1.28.0
  environment:
    name: staging
    url: https://staging.docuthinker.com
  before_script:
    - echo "$KUBE_CONFIG_STAGING" | base64 -d > $KUBECONFIG
    - kubectl config use-context staging-cluster
  script:
    - kubectl set image deployment/frontend frontend=$REGISTRY/frontend:$CI_COMMIT_SHA -n docuthinker-staging
    - kubectl set image deployment/backend backend=$REGISTRY/backend:$CI_COMMIT_SHA -n docuthinker-staging
    - kubectl rollout status deployment/frontend -n docuthinker-staging --timeout=5m
    - kubectl rollout status deployment/backend -n docuthinker-staging --timeout=5m
  only:
    - master
  when: manual

deploy-production:
  stage: deploy-production
  image: alpine/k8s:1.28.0
  environment:
    name: production
    url: https://docuthinker.com
  before_script:
    - echo "$KUBE_CONFIG_PROD" | base64 -d > $KUBECONFIG
    - kubectl config use-context prod-cluster
  script:
    - echo "Deploying to production with canary strategy"
    - kubectl set image deployment/backend-canary backend=$REGISTRY/backend:$CI_COMMIT_SHA -n docuthinker-prod
    - kubectl set image deployment/frontend-canary frontend=$REGISTRY/frontend:$CI_COMMIT_SHA -n docuthinker-prod
    - kubectl rollout status deployment/backend-canary -n docuthinker-prod --timeout=5m
    - kubectl rollout status deployment/frontend-canary -n docuthinker-prod --timeout=5m
    - echo "Canary deployment successful. Please verify metrics before promoting."
  only:
    - master
  when: manual

promote-production:
  stage: deploy-production
  image: alpine/k8s:1.28.0
  environment:
    name: production
    url: https://docuthinker.com
  before_script:
    - echo "$KUBE_CONFIG_PROD" | base64 -d > $KUBECONFIG
    - kubectl config use-context prod-cluster
  script:
    - kubectl set image deployment/backend-blue backend=$REGISTRY/backend:$CI_COMMIT_SHA -n docuthinker-prod
    - kubectl set image deployment/frontend-blue frontend=$REGISTRY/frontend:$CI_COMMIT_SHA -n docuthinker-prod
    - kubectl rollout status deployment/backend-blue -n docuthinker-prod --timeout=5m
    - kubectl rollout status deployment/frontend-blue -n docuthinker-prod --timeout=5m
    - kubectl patch service backend-service -p '{"spec":{"selector":{"track":"blue"}}}' -n docuthinker-prod
    - kubectl patch service frontend-service -p '{"spec":{"selector":{"track":"blue"}}}' -n docuthinker-prod
  only:
    - master
  when: manual
  needs:
    - deploy-production

# ============== POST-DEPLOY STAGE ==============

smoke-test-dev:
  stage: post-deploy
  image: curlimages/curl:latest
  script:
    - sleep 30
    - curl -f https://dev.docuthinker.com/health || exit 1
    - curl -f https://dev.docuthinker.com/api/health || exit 1
  only:
    - develop
  needs:
    - deploy-dev

smoke-test-staging:
  stage: post-deploy
  image: curlimages/curl:latest
  script:
    - sleep 30
    - curl -f https://staging.docuthinker.com/health || exit 1
    - curl -f https://staging.docuthinker.com/api/health || exit 1
  only:
    - master
  needs:
    - deploy-staging
  when: on_success

smoke-test-production:
  stage: post-deploy
  image: curlimages/curl:latest
  script:
    - sleep 30
    - curl -f https://docuthinker.com/health || exit 1
    - curl -f https://docuthinker.com/api/health || exit 1
  only:
    - master
  needs:
    - promote-production
  when: on_success

performance-test:
  stage: post-deploy
  image: grafana/k6:latest
  script:
    - k6 run --vus 10 --duration 30s scripts/performance/load-test.js
  artifacts:
    paths:
      - k6-results.json
    expire_in: 7 days
  allow_failure: true
  only:
    - master
  needs:
    - deploy-staging
  when: manual

# ============== CLEANUP STAGE ==============

cleanup-old-images:
  stage: cleanup
  image: alpine:latest
  script:
    - echo "Cleanup old images from registry"
    - apk add --no-cache curl jq
    - echo "Image cleanup script would run here"
  only:
    - master
  when: on_success
  allow_failure: true
