{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete AI/ML Pipeline with ONNX and Extended Features\n",
        "\n",
        "Welcome to the **Complete AI/ML Pipeline** notebook! This notebook:\n",
        "\n",
        "- Demonstrates a **production-ready** system for **document analysis** (summarization, topic extraction, translation, sentiment, Q&A, discussion generation, RAG, etc.)\n",
        "- Includes **extended features** such as key ideas extraction, bullet-point summaries, chat interface, voice chat stubs, rewriting, recommendations, and summary refinement.\n",
        "- Showcases how to **convert** models to ONNX format (via `convert_to_onnx.py`), load them dynamically, and fall back to standard PyTorch pipelines if ONNX models are not found.\n",
        "- Provides a **Dockerfile** and a **FastAPI** server (`server.py`) for easy deployment.\n",
        "\n",
        "This notebook is divided into the following sections:\n",
        "1. **Setup and Requirements**\n",
        "2. **Project Structure**\n",
        "3. **convert_to_onnx.py** Script\n",
        "4. **Core Code** (the `ai_ml` package)\n",
        "5. **Extended Features**\n",
        "6. **Dockerfile and requirements.txt**\n",
        "7. **Optional: FastAPI server.py**\n",
        "8. **Usage Instructions**\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Requirements\n",
        "\n",
        "Before running this notebook or using the code, install the following:\n",
        "```bash\n",
        "pip install transformers torch langchain onnx onnxruntime python-dotenv fastapi uvicorn\n",
        "```\n",
        "Make sure your environment is properly set up (Python 3.9+ recommended)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Project Structure\n",
        "\n",
        "Here is the recommended layout for this project:\n",
        "```\n",
        ".\n",
        "├── convert_to_onnx.py\n",
        "├── Dockerfile\n",
        "├── requirements.txt\n",
        "├── server.py\n",
        "├── ai_ml/\n",
        "│   ├── __init__.py\n",
        "│   ├── config.py\n",
        "│   ├── main.py\n",
        "│   ├── backend.py\n",
        "│   ├── models/\n",
        "│   │   ├── __init__.py\n",
        "│   │   ├── hf_model.py\n",
        "│   │   ├── model_utils.py\n",
        "│   │   └── onnx_helper.py\n",
        "│   ├── processing/\n",
        "│   │   ├── __init__.py\n",
        "│   │   ├── summarizer.py\n",
        "│   │   ├── translator.py\n",
        "│   │   ├── topic_extractor.py\n",
        "│   │   └── sentiment.py\n",
        "│   ├── qa/\n",
        "│   │   ├── __init__.py\n",
        "│   │   └── qa_system.py\n",
        "│   ├── discussion/\n",
        "│   │   ├── __init__.py\n",
        "│   │   └── discussion_generator.py\n",
        "│   ├── rag/\n",
        "│   │   ├── __init__.py\n",
        "│   │   └── rag_system.py\n",
        "│   └── extended_features/\n",
        "│       ├── __init__.py\n",
        "│       ├── key_ideas_extractor.py\n",
        "│       ├── bullet_summary_generator.py\n",
        "│       ├── chat_interface.py\n",
        "│       ├── voice_chat.py\n",
        "│       ├── rewriter.py\n",
        "│       ├── recommendations_generator.py\n",
        "│       └── refine_summary.py\n",
        "```\n",
        "\n",
        "In this notebook, we'll show all files. You can copy them into this structure. Then, you can optionally run the `convert_to_onnx.py` script if you want ONNX conversions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. `convert_to_onnx.py` Script\n",
        "\n",
        "This script automates the conversion of all needed Hugging Face models to ONNX format. It uses the built-in transformers CLI. Adjust or remove what you don't need.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile convert_to_onnx.py\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def run_conversion(model_name, feature, output_dir):\n",
        "    \"\"\"\n",
        "    Runs the ONNX export command for a given model, feature, and output directory.\n",
        "    \"\"\"\n",
        "    command = [\n",
        "        sys.executable, \"-m\", \"transformers.onnx\",\n",
        "        \"--model\", model_name,\n",
        "        \"--feature\", feature,\n",
        "        output_dir\n",
        "    ]\n",
        "    print(\"Running command:\", \" \".join(command))\n",
        "    subprocess.run(command, check=True)\n",
        "\n",
        "def main():\n",
        "    # Create base directories for ONNX models\n",
        "    base_dirs = [\n",
        "        \"onnx_models/summarizer\",\n",
        "        \"onnx_models/qa\",\n",
        "        \"onnx_models/discussion\",\n",
        "        \"onnx_models/rag\",\n",
        "        \"onnx_models/sentiment\",\n",
        "    ]\n",
        "    \n",
        "    # Translation model mappings (language code to model name)\n",
        "    translations = {\n",
        "        \"fr\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
        "        \"de\": \"Helsinki-NLP/opus-mt-en-de\",\n",
        "        \"es\": \"Helsinki-NLP/opus-mt-en-es\",\n",
        "        \"it\": \"Helsinki-NLP/opus-mt-en-it\",\n",
        "        \"zh\": \"Helsinki-NLP/opus-mt-en-zh\",\n",
        "    }\n",
        "    \n",
        "    for dir_path in base_dirs:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "    \n",
        "    # Create directories for translation models\n",
        "    for lang in translations.keys():\n",
        "        os.makedirs(f\"onnx_models/translation/{lang}\", exist_ok=True)\n",
        "    \n",
        "    try:\n",
        "        # Convert Summarizer model\n",
        "        run_conversion(\"facebook/bart-large-cnn\", \"summarization\", \"onnx_models/summarizer\")\n",
        "        \n",
        "        # Convert QA model\n",
        "        run_conversion(\"distilbert-base-cased-distilled-squad\", \"question-answering\", \"onnx_models/qa\")\n",
        "        \n",
        "        # Convert Discussion Generator model (using GPT-2 for text generation)\n",
        "        run_conversion(\"gpt2\", \"text-generation\", \"onnx_models/discussion\")\n",
        "        \n",
        "        # Convert RAG model (for text2text-generation)\n",
        "        run_conversion(\"facebook/rag-token-nq\", \"text2text-generation\", \"onnx_models/rag\")\n",
        "        \n",
        "        # Convert Sentiment Analysis model\n",
        "        run_conversion(\"distilbert-base-uncased-finetuned-sst-2-english\", \"sequence-classification\", \"onnx_models/sentiment\")\n",
        "        \n",
        "        # Convert Translation models for each target language\n",
        "        for lang, model in translations.items():\n",
        "            run_conversion(model, \"translation\", f\"onnx_models/translation/{lang}\")\n",
        "        \n",
        "        print(\"All models have been successfully converted to ONNX format!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"An error occurred during ONNX conversion:\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Core Code: `ai_ml` Package\n",
        "\n",
        "We'll write out each file. In a real environment, place them under the `ai_ml/` directory as described."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/__init__.py\n",
        "# This file can remain empty or include package-wide initializations.\n",
        "pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/config.py\n",
        "# Model names for Hugging Face pipelines\n",
        "MODEL_NAMES = {\n",
        "    \"summarizer\": \"facebook/bart-large-cnn\",\n",
        "    \"qa\": \"distilbert-base-cased-distilled-squad\",\n",
        "    \"discussion\": \"gpt2\",\n",
        "    \"rag\": \"facebook/rag-token-nq\",\n",
        "    \"topic_extractor\": \"facebook/bart-large-mnli\"\n",
        "    # DistilBERT sentiment model is loaded in hf_model.py\n",
        "}\n",
        "\n",
        "# Translation model mapping\n",
        "TRANSLATION_MODELS = {\n",
        "    \"fr\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
        "    \"de\": \"Helsinki-NLP/opus-mt-en-de\",\n",
        "    \"es\": \"Helsinki-NLP/opus-mt-en-es\",\n",
        "    \"it\": \"Helsinki-NLP/opus-mt-en-it\",\n",
        "    \"zh\": \"Helsinki-NLP/opus-mt-en-zh\"\n",
        "}\n",
        "\n",
        "# Prompt templates for LangChain\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"summarization\": \"Summarize the following document in a concise manner:\\n\\n{text}\\n\\nSummary:\",\n",
        "    \"qa\": \"Based on the context below, answer the question:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
        "    \"discussion\": \"Generate discussion points for the following document:\\n\\n{text}\\n\\nDiscussion Points:\",\n",
        "    \"rag\": \"Analyze the following document and provide an in-depth analysis:\\n\\n{text}\\n\\nAnalysis:\"\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/main.py\n",
        "#!/usr/bin/env python\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "from models.hf_model import load_models, load_translation_model\n",
        "from processing.summarizer import summarize_text\n",
        "from processing.topic_extractor import extract_topics\n",
        "from processing.translator import translate_text\n",
        "from processing.sentiment import analyze_sentiment\n",
        "from qa.qa_system import answer_question\n",
        "from discussion.discussion_generator import generate_discussion_points\n",
        "from rag.rag_system import retrieval_augmented_generation\n",
        "\n",
        "def setup_logging():\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)]\n",
        "    )\n",
        "\n",
        "def main():\n",
        "    setup_logging()\n",
        "    logger = logging.getLogger(__name__)\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Document Analysis App with LangChain Integration\"\n",
        "    )\n",
        "    parser.add_argument(\"filepath\", help=\"Path to the document file (txt) to analyze\")\n",
        "    parser.add_argument(\"--question\", help=\"Question for Q&A\", default=None)\n",
        "    parser.add_argument(\n",
        "        \"--translate_lang\",\n",
        "        help=\"Target language code for translation (e.g., 'fr', 'de', 'es', 'it', 'zh')\",\n",
        "        default=\"fr\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    try:\n",
        "        with open(args.filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            document = f.read()\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error reading file: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load LangChain chains and pipelines\n",
        "    logger.info(\"Loading main models and chains...\")\n",
        "    models = load_models()\n",
        "\n",
        "    # Dynamically load the translation pipeline for the specified target language\n",
        "    try:\n",
        "        translator = load_translation_model(args.translate_lang)\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error loading translation model: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Summarize the document (with chunking for long texts)\n",
        "    try:\n",
        "        summary = summarize_text(document, models[\"summarizer_chain\"])\n",
        "        print(\"=== Summary ===\")\n",
        "        print(summary)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during summarization: %s\", e)\n",
        "\n",
        "    # Extract key topics using the zero-shot classification pipeline\n",
        "    try:\n",
        "        topics = extract_topics(document, models[\"topic_extractor\"])\n",
        "        print(\"\\n=== Key Topics ===\")\n",
        "        print(topics)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during topic extraction: %s\", e)\n",
        "\n",
        "    # Translate the document\n",
        "    try:\n",
        "        translation = translate_text(document, translator)\n",
        "        print(\"\\n=== Translated Document (to {}) ===\".format(args.translate_lang))\n",
        "        print(translation)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during translation: %s\", e)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    try:\n",
        "        sentiment = analyze_sentiment(document, models[\"sentiment_analyzer\"])\n",
        "        print(\"\\n=== Sentiment Analysis ===\")\n",
        "        print(sentiment)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during sentiment analysis: %s\", e)\n",
        "\n",
        "    # Q&A (if a question is provided)\n",
        "    if args.question:\n",
        "        try:\n",
        "            answer = answer_question(document, args.question, models[\"qa_chain\"])\n",
        "            print(\"\\n=== Q&A Answer ===\")\n",
        "            print(answer)\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Error during Q&A: %s\", e)\n",
        "\n",
        "    # Generate discussion points\n",
        "    try:\n",
        "        discussion = generate_discussion_points(document, models[\"discussion_chain\"])\n",
        "        print(\"\\n=== Discussion Points ===\")\n",
        "        print(discussion)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during discussion generation: %s\", e)\n",
        "\n",
        "    # Demonstrate RAG\n",
        "    try:\n",
        "        rag_answer = retrieval_augmented_generation(document, models[\"rag_chain\"])\n",
        "        print(\"\\n=== RAG Generated Answer ===\")\n",
        "        print(rag_answer)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during RAG generation: %s\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/backend.py\n",
        "import logging\n",
        "from models.hf_model import load_models, load_translation_model\n",
        "from processing.summarizer import summarize_text\n",
        "from processing.topic_extractor import extract_topics\n",
        "from processing.translator import translate_text\n",
        "from processing.sentiment import analyze_sentiment\n",
        "from qa.qa_system import answer_question\n",
        "from discussion.discussion_generator import generate_discussion_points\n",
        "from rag.rag_system import retrieval_augmented_generation\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Cache models to avoid reloading on each call\n",
        "MODELS = None\n",
        "\n",
        "def initialize_models():\n",
        "    global MODELS\n",
        "    if MODELS is None:\n",
        "        logger.info(\"Loading models and chains...\")\n",
        "        MODELS = load_models()\n",
        "    return MODELS\n",
        "\n",
        "def analyze_document(document: str, question: str = None, translate_lang: str = \"fr\"):\n",
        "    \"\"\"\n",
        "    Analyze the provided document and return a dictionary with the results.\n",
        "\n",
        "    Parameters:\n",
        "      document (str): The document text.\n",
        "      question (str): Optional question for Q&A.\n",
        "      translate_lang (str): The target language code for translation.\n",
        "\n",
        "    Returns:\n",
        "      dict: Contains keys: summary, topics, translation, sentiment, qa, discussion, rag.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    models = initialize_models()\n",
        "\n",
        "    # Load translation pipeline for the specified target language.\n",
        "    try:\n",
        "        translator = load_translation_model(translate_lang)\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error loading translation model: %s\", e)\n",
        "        translator = None\n",
        "\n",
        "    # Summarization\n",
        "    try:\n",
        "        summary = summarize_text(document, models[\"summarizer_chain\"])\n",
        "        results[\"summary\"] = summary\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during summarization: %s\", e)\n",
        "        results[\"summary\"] = None\n",
        "\n",
        "    # Topic extraction\n",
        "    try:\n",
        "        topics = extract_topics(document, models[\"topic_extractor\"])\n",
        "        results[\"topics\"] = topics\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during topic extraction: %s\", e)\n",
        "        results[\"topics\"] = None\n",
        "\n",
        "    # Translation\n",
        "    try:\n",
        "        if translator:\n",
        "            translation = translate_text(document, translator)\n",
        "            results[\"translation\"] = translation\n",
        "        else:\n",
        "            results[\"translation\"] = None\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during translation: %s\", e)\n",
        "        results[\"translation\"] = None\n",
        "\n",
        "    # Sentiment\n",
        "    try:\n",
        "        sentiment = analyze_sentiment(document, models[\"sentiment_analyzer\"])\n",
        "        results[\"sentiment\"] = sentiment\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during sentiment analysis: %s\", e)\n",
        "        results[\"sentiment\"] = None\n",
        "\n",
        "    # Q&A\n",
        "    if question:\n",
        "        try:\n",
        "            answer = answer_question(document, question, models[\"qa_chain\"])\n",
        "            results[\"qa\"] = answer\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Error during Q&A: %s\", e)\n",
        "            results[\"qa\"] = None\n",
        "    else:\n",
        "        results[\"qa\"] = None\n",
        "\n",
        "    # Discussion\n",
        "    try:\n",
        "        discussion = generate_discussion_points(document, models[\"discussion_chain\"])\n",
        "        results[\"discussion\"] = discussion\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during discussion generation: %s\", e)\n",
        "        results[\"discussion\"] = None\n",
        "\n",
        "    # RAG\n",
        "    try:\n",
        "        rag_answer = retrieval_augmented_generation(document, models[\"rag_chain\"])\n",
        "        results[\"rag\"] = rag_answer\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during RAG generation: %s\", e)\n",
        "        results[\"rag\"] = None\n",
        "\n",
        "    return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `ai_ml/models/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/models/__init__.py\n",
        "from .hf_model import load_models, load_translation_model\n",
        "from .model_utils import time_function, postprocess_text, ensemble_outputs, safe_execute\n",
        "from .onnx_helper import check_onnx_model_exists, get_onnx_model_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/models/hf_model.py\n",
        "import os\n",
        "import logging\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from ai_ml.config import MODEL_NAMES, TRANSLATION_MODELS, PROMPT_TEMPLATES\n",
        "from .onnx_helper import check_onnx_model_exists, get_onnx_model_path\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if we should use ONNX models\n",
        "USE_ONNX = os.getenv(\"USE_ONNX\", \"false\").lower() == \"true\"\n",
        "\n",
        "def load_models():\n",
        "    models = {}\n",
        "    \n",
        "    # Summarizer chain\n",
        "    try:\n",
        "        logger.info(\"Loading summarizer pipeline...\")\n",
        "        summarizer_chain = _create_seq2seq_chain(\n",
        "            model_name=MODEL_NAMES[\"summarizer\"],\n",
        "            onnx_subdir=\"summarizer\",\n",
        "            prompt_template=PROMPT_TEMPLATES[\"summarization\"]\n",
        "        )\n",
        "        models[\"summarizer_chain\"] = summarizer_chain\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error loading summarization chain: %s\", e)\n",
        "        raise e\n",
        "\n",
        "    # QA chain\n",
        "    try:\n",
        "        logger.info(\"Loading QA pipeline...\")\n",
        "        qa_chain = _create_qa_chain(\n",
        "            model_name=MODEL_NAMES[\"qa\"],\n",
        "            onnx_subdir=\"qa\",\n",
        "            prompt_template=PROMPT_TEMPLATES[\"qa\"]\n",
        "        )\n",
        "        models[\"qa_chain\"] = qa_chain\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error loading QA chain: %s\", e)\n",
        "        raise e\n",
        "\n",
        "    # Discussion chain\n",
        "    try:\n",
        "        logger.info(\"Loading discussion generator pipeline...\")\n",
        "        discussion_chain = _create_textgen_chain(\n",
        "            model_name=MODEL_NAMES[\"discussion\"],\n",
        "            onnx_subdir=\"discussion\",\n",
        "            prompt_template=PROMPT_TEMPLATES[\"discussion\"]\n",
        "        )\n",
        "        models[\"discussion_chain\"] = discussion_chain\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error loading discussion chain: %s\", e)\n",
        "        raise e\n",
        "\n",
        "    # RAG chain\n",
        "    try:\n",
        "        logger.info(\"Loading RAG pipeline...\")\n",
        "        rag_chain = _create_text2text_chain(\n",
        "            model_name=MODEL_NAMES[\"rag\"],\n",
        "            onnx_subdir=\"rag\",\n",
        "            prompt_template=PROMPT_TEMPLATES[\"rag\"]\n",
        "        )\n",
        "        models[\"rag_chain\"] = rag_chain\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error loading RAG chain: %s\", e)\n",
        "        raise e\n",
        "\n",
        "    # Topic extractor\n",
        "    try:\n",
        "        logger.info(\"Loading topic extraction pipeline...\")\n",
        "        models[\"topic_extractor\"] = pipeline(\"zero-shot-classification\", model=MODEL_NAMES[\"topic_extractor\"])\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error loading topic extractor: %s\", e)\n",
        "        raise e\n",
        "\n",
        "    # Sentiment analyzer\n",
        "    try:\n",
        "        logger.info(\"Loading sentiment analyzer pipeline...\")\n",
        "        sentiment_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "        if USE_ONNX:\n",
        "            onnx_path = get_onnx_model_path(\"sentiment\")\n",
        "            if check_onnx_model_exists(onnx_path):\n",
        "                tokenizer = AutoTokenizer.from_pretrained(sentiment_model)\n",
        "                models[\"sentiment_analyzer\"] = pipeline(\n",
        "                    \"sentiment-analysis\",\n",
        "                    model=onnx_path,\n",
        "                    tokenizer=tokenizer,\n",
        "                    framework=\"onnxruntime\"\n",
        "                )\n",
        "            else:\n",
        "                models[\"sentiment_analyzer\"] = pipeline(\"sentiment-analysis\", model=sentiment_model)\n",
        "        else:\n",
        "            models[\"sentiment_analyzer\"] = pipeline(\"sentiment-analysis\", model=sentiment_model)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error loading sentiment analyzer: %s\", e)\n",
        "        raise e\n",
        "\n",
        "    return models\n",
        "\n",
        "def load_translation_model(target_lang):\n",
        "    \"\"\"\n",
        "    Dynamically load a translation pipeline for the specified target language.\n",
        "    \"\"\"\n",
        "    if target_lang not in TRANSLATION_MODELS:\n",
        "        raise ValueError(f\"Translation model for language '{target_lang}' is not configured.\")\n",
        "    \n",
        "    model_name = TRANSLATION_MODELS[target_lang]\n",
        "    task_name = f\"translation_en_to_{target_lang}\"\n",
        "    logger.info(\"Loading translator model for language '%s'...\", target_lang)\n",
        "\n",
        "    if USE_ONNX:\n",
        "        from transformers import AutoTokenizer\n",
        "        onnx_path = get_onnx_model_path(\"translation\", target_lang)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if check_onnx_model_exists(onnx_path):\n",
        "            translator = pipeline(\n",
        "                task_name,\n",
        "                model=onnx_path,\n",
        "                tokenizer=tokenizer,\n",
        "                framework=\"onnxruntime\"\n",
        "            )\n",
        "        else:\n",
        "            translator = pipeline(task_name, model=model_name)\n",
        "    else:\n",
        "        translator = pipeline(task_name, model=model_name)\n",
        "    return translator\n",
        "\n",
        "def _create_seq2seq_chain(model_name, onnx_subdir, prompt_template):\n",
        "    \"\"\"\n",
        "    Creates a seq2seq chain (e.g. summarization).\n",
        "    \"\"\"\n",
        "    if USE_ONNX:\n",
        "        onnx_path = get_onnx_model_path(onnx_subdir)\n",
        "        if check_onnx_model_exists(onnx_path):\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            pipe = pipeline(\"summarization\", model=onnx_path, tokenizer=tokenizer, framework=\"onnxruntime\")\n",
        "        else:\n",
        "            pipe = pipeline(\"summarization\", model=model_name)\n",
        "    else:\n",
        "        pipe = pipeline(\"summarization\", model=model_name)\n",
        "    \n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    template = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "    return LLMChain(llm=llm, prompt=template)\n",
        "\n",
        "def _create_qa_chain(model_name, onnx_subdir, prompt_template):\n",
        "    \"\"\"\n",
        "    Creates a QA chain.\n",
        "    \"\"\"\n",
        "    if USE_ONNX:\n",
        "        onnx_path = get_onnx_model_path(onnx_subdir)\n",
        "        if check_onnx_model_exists(onnx_path):\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            pipe = pipeline(\"question-answering\", model=onnx_path, tokenizer=tokenizer, framework=\"onnxruntime\")\n",
        "        else:\n",
        "            pipe = pipeline(\"question-answering\", model=model_name)\n",
        "    else:\n",
        "        pipe = pipeline(\"question-answering\", model=model_name)\n",
        "    \n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    template = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "    return LLMChain(llm=llm, prompt=template)\n",
        "\n",
        "def _create_textgen_chain(model_name, onnx_subdir, prompt_template):\n",
        "    \"\"\"\n",
        "    Creates a text-generation chain (e.g. for discussion).\n",
        "    \"\"\"\n",
        "    if USE_ONNX:\n",
        "        onnx_path = get_onnx_model_path(onnx_subdir)\n",
        "        if check_onnx_model_exists(onnx_path):\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            pipe = pipeline(\"text-generation\", model=onnx_path, tokenizer=tokenizer, framework=\"onnxruntime\")\n",
        "        else:\n",
        "            pipe = pipeline(\"text-generation\", model=model_name)\n",
        "    else:\n",
        "        pipe = pipeline(\"text-generation\", model=model_name)\n",
        "    \n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    template = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "    return LLMChain(llm=llm, prompt=template)\n",
        "\n",
        "def _create_text2text_chain(model_name, onnx_subdir, prompt_template):\n",
        "    \"\"\"\n",
        "    Creates a text2text-generation chain (e.g. RAG).\n",
        "    \"\"\"\n",
        "    if USE_ONNX:\n",
        "        onnx_path = get_onnx_model_path(onnx_subdir)\n",
        "        if check_onnx_model_exists(onnx_path):\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            pipe = pipeline(\"text2text-generation\", model=onnx_path, tokenizer=tokenizer, framework=\"onnxruntime\")\n",
        "        else:\n",
        "            pipe = pipeline(\"text2text-generation\", model=model_name)\n",
        "    else:\n",
        "        pipe = pipeline(\"text2text-generation\", model=model_name)\n",
        "    \n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    template = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "    return LLMChain(llm=llm, prompt=template)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/models/model_utils.py\n",
        "import logging\n",
        "import time\n",
        "from typing import Callable, Any, Tuple\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def time_function(func: Callable, *args, **kwargs) -> Tuple[Any, float]:\n",
        "    \"\"\"\n",
        "    Times the execution of a function.\n",
        "    Returns (result, elapsed_time).\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    logger.info(\"Function '%s' executed in %.2f seconds\", func.__name__, elapsed_time)\n",
        "    return result, elapsed_time\n",
        "\n",
        "def postprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs basic post-processing on text (trimming, ensuring period at end).\n",
        "    \"\"\"\n",
        "    if text:\n",
        "        text = \" \".join(text.strip().split())\n",
        "        if not text.endswith('.'):\n",
        "            text += '.'\n",
        "    return text\n",
        "\n",
        "def ensemble_outputs(outputs: list, method: str = \"first\") -> str:\n",
        "    \"\"\"\n",
        "    Combines outputs from multiple models. Currently supports \"first\" approach.\n",
        "    \"\"\"\n",
        "    if not outputs:\n",
        "        return \"\"\n",
        "    if method == \"first\":\n",
        "        return outputs[0]\n",
        "    return outputs[0]\n",
        "\n",
        "def safe_execute(func: Callable, *args, **kwargs) -> Tuple[Any, Exception]:\n",
        "    \"\"\"\n",
        "    Safely executes a function and captures exceptions.\n",
        "    Returns (result, error).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = func(*args, **kwargs)\n",
        "        return result, None\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error executing function '%s': %s\", func.__name__, e)\n",
        "        return None, e\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/models/onnx_helper.py\n",
        "import os\n",
        "\n",
        "def check_onnx_model_exists(model_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if a model.onnx file exists in the given directory.\n",
        "    \"\"\"\n",
        "    onnx_file = os.path.join(model_path, \"model.onnx\")\n",
        "    return os.path.exists(onnx_file)\n",
        "\n",
        "def get_onnx_model_path(task: str, extra: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Constructs the path to an ONNX model for a given task.\n",
        "    If extra is provided (e.g., a language code), it's appended.\n",
        "    \"\"\"\n",
        "    base_dir = \"onnx_models\"\n",
        "    if extra:\n",
        "        return os.path.join(base_dir, task, extra)\n",
        "    return os.path.join(base_dir, task)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `ai_ml/processing/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/processing/__init__.py\n",
        "from .summarizer import summarize_text\n",
        "from .translator import translate_text\n",
        "from .topic_extractor import extract_topics\n",
        "from .sentiment import analyze_sentiment\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/processing/summarizer.py\n",
        "def summarize_text(text, summarizer_chain, max_chunk_length=1000):\n",
        "    \"\"\"\n",
        "    Summarizes the given text using the provided summarizer chain.\n",
        "    If the text exceeds max_chunk_length, it is split into chunks.\n",
        "    \"\"\"\n",
        "    if len(text) <= max_chunk_length:\n",
        "        chunks = [text]\n",
        "    else:\n",
        "        chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
        "    \n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer_chain.run(text=chunk)\n",
        "        summaries.append(summary.strip())\n",
        "    return \" \".join(summaries)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/processing/translator.py\n",
        "def translate_text(text, translator, max_length=512):\n",
        "    \"\"\"\n",
        "    Translates the text using the provided translator pipeline.\n",
        "    \"\"\"\n",
        "    if not translator:\n",
        "        return \"\"\n",
        "    translation_result = translator(text, max_length=max_length)\n",
        "    return translation_result[0][\"translation_text\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/processing/topic_extractor.py\n",
        "def extract_topics(text, topic_extractor):\n",
        "    \"\"\"\n",
        "    Extracts key topics from the text using zero-shot classification.\n",
        "    \"\"\"\n",
        "    candidate_labels = [\n",
        "        \"technology\", \"health\", \"finance\", \"politics\", \n",
        "        \"sports\", \"education\", \"science\", \"entertainment\"\n",
        "    ]\n",
        "    result = topic_extractor(text, candidate_labels)\n",
        "    # Sort by score, return top three\n",
        "    topics = sorted(zip(result[\"labels\"], result[\"scores\"]), key=lambda x: x[1], reverse=True)\n",
        "    return [topic for topic, score in topics[:3]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/processing/sentiment.py\n",
        "def analyze_sentiment(text, sentiment_analyzer):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of the text using the sentiment analysis pipeline.\n",
        "    \"\"\"\n",
        "    result = sentiment_analyzer(text)\n",
        "    return result[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `ai_ml/qa/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/qa/__init__.py\n",
        "from .qa_system import answer_question\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/qa/qa_system.py\n",
        "def answer_question(context, question, qa_chain):\n",
        "    \"\"\"\n",
        "    Answers the question based on the context using the QA chain.\n",
        "    \"\"\"\n",
        "    answer = qa_chain.run(context=context, question=question)\n",
        "    return answer.strip()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `ai_ml/discussion/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/discussion/__init__.py\n",
        "from .discussion_generator import generate_discussion_points\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/discussion/discussion_generator.py\n",
        "def generate_discussion_points(text, discussion_chain):\n",
        "    \"\"\"\n",
        "    Generates discussion points for the given text using a discussion chain.\n",
        "    \"\"\"\n",
        "    result = discussion_chain.run(text=text)\n",
        "    return result.strip()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `ai_ml/rag/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/rag/__init__.py\n",
        "from .rag_system import retrieval_augmented_generation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/rag/rag_system.py\n",
        "def retrieval_augmented_generation(text, rag_chain):\n",
        "    \"\"\"\n",
        "    Demonstrates a retrieval-augmented generation approach (RAG).\n",
        "    \"\"\"\n",
        "    result = rag_chain.run(text=text)\n",
        "    return result.strip()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extended Features\n",
        "\n",
        "Below are the modules for key ideas, bullet-point summaries, chat interface, voice chat stubs, rewriting, recommendations, and summary refinement. Place them in `ai_ml/extended_features/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/__init__.py\n",
        "# This file can remain empty or import extended feature functions.\n",
        "pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/key_ideas_extractor.py\n",
        "import logging\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "KEY_IDEAS_PROMPT = \"\"\"\n",
        "Extract the main ideas (key takeaways) from the following document. Present them as a concise list:\n",
        "Document:\n",
        "{text}\n",
        "\n",
        "Key Ideas:\n",
        "\"\"\"\n",
        "\n",
        "def generate_key_ideas(document: str, chain: LLMChain) -> str:\n",
        "    \"\"\"\n",
        "    Generates the key ideas from the document using a specialized LLMChain.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        template = PromptTemplate(input_variables=[\"text\"], template=KEY_IDEAS_PROMPT)\n",
        "        local_chain = LLMChain(llm=chain.llm, prompt=template)\n",
        "        result = local_chain.run(text=document)\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error generating key ideas: %s\", e)\n",
        "        return \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/bullet_summary_generator.py\n",
        "import logging\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "BULLET_SUMMARY_PROMPT = \"\"\"\n",
        "Please summarize the following document in bullet points:\n",
        "Document:\n",
        "{text}\n",
        "\n",
        "Bullet-Point Summary:\n",
        "\"\"\"\n",
        "\n",
        "def generate_bullet_summary(document: str, chain: LLMChain) -> str:\n",
        "    \"\"\"\n",
        "    Generates a bullet-point summary of the document.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        template = PromptTemplate(input_variables=[\"text\"], template=BULLET_SUMMARY_PROMPT)\n",
        "        local_chain = LLMChain(llm=chain.llm, prompt=template)\n",
        "        result = local_chain.run(text=document)\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error generating bullet summary: %s\", e)\n",
        "        return \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/chat_interface.py\n",
        "import logging\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def chat_with_ai(user_input: str, conversation_chain: ConversationChain) -> str:\n",
        "    \"\"\"\n",
        "    Allows a user to chat with the AI. The conversation_chain should maintain history.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = conversation_chain.run(input=user_input)\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error in chat interface: %s\", e)\n",
        "        return \"An error occurred.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/voice_chat.py\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def voice_to_text(audio_data: bytes) -> str:\n",
        "    \"\"\"\n",
        "    Convert the audio_data to text (STT). Stub implementation.\n",
        "    \"\"\"\n",
        "    # e.g. use a whisper pipeline or an external service\n",
        "    text = \"STUB: recognized text from audio\"\n",
        "    return text\n",
        "\n",
        "def text_to_voice(text: str) -> bytes:\n",
        "    \"\"\"\n",
        "    Convert text to audio bytes (TTS). Stub implementation.\n",
        "    \"\"\"\n",
        "    # e.g. use a TTS library or huggingface TTS pipeline\n",
        "    audio_data = b\"STUB: audio bytes\"\n",
        "    return audio_data\n",
        "\n",
        "def voice_chat(audio_data: bytes, conversation_chain) -> bytes:\n",
        "    \"\"\"\n",
        "    1. Convert user audio to text\n",
        "    2. Pass text to conversation chain\n",
        "    3. Convert AI response to audio\n",
        "    \"\"\"\n",
        "    try:\n",
        "        user_text = voice_to_text(audio_data)\n",
        "        ai_response = conversation_chain.run(input=user_text)\n",
        "        response_audio = text_to_voice(ai_response)\n",
        "        return response_audio\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error in voice chat: %s\", e)\n",
        "        return b\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/rewriter.py\n",
        "import logging\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "REWRITE_PROMPT = \"\"\"\n",
        "Rewrite the following text in a clear, coherent, and concise manner, preserving its meaning:\n",
        "{text}\n",
        "\n",
        "Rewritten:\n",
        "\"\"\"\n",
        "\n",
        "def rewrite_content(document: str, rewrite_chain: LLMChain) -> str:\n",
        "    \"\"\"\n",
        "    Rewrites content using a text2text-generation or similar chain.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        template = PromptTemplate(input_variables=[\"text\"], template=REWRITE_PROMPT)\n",
        "        local_chain = LLMChain(llm=rewrite_chain.llm, prompt=template)\n",
        "        result = local_chain.run(text=document)\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error rewriting content: %s\", e)\n",
        "        return \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/recommendations_generator.py\n",
        "import logging\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "RECOMMENDATIONS_PROMPT = \"\"\"\n",
        "Read the document below and provide actionable recommendations or next steps:\n",
        "Document:\n",
        "{text}\n",
        "\n",
        "Recommendations:\n",
        "\"\"\"\n",
        "\n",
        "def generate_recommendations(document: str, recommendation_chain: LLMChain) -> str:\n",
        "    \"\"\"\n",
        "    Generates recommendations or next steps based on the document.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        template = PromptTemplate(input_variables=[\"text\"], template=RECOMMENDATIONS_PROMPT)\n",
        "        local_chain = LLMChain(llm=recommendation_chain.llm, prompt=template)\n",
        "        result = local_chain.run(text=document)\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error generating recommendations: %s\", e)\n",
        "        return \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%writefile ai_ml/extended_features/refine_summary.py\n",
        "import logging\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "REFINE_SUMMARY_PROMPT = \"\"\"\n",
        "We have an original document and an existing summary. Refine the summary to ensure it is accurate, clear, and concise:\n",
        "Document:\n",
        "{document}\n",
        "\n",
        "Existing Summary:\n",
        "{summary}\n",
        "\n",
        "Refined Summary:\n",
        "\"\"\"\n",
        "\n",
        "def refine_summary(document: str, existing_summary: str, refine_chain: LLMChain) -> str:\n",
        "    \"\"\"\n",
        "    Refines the existing summary by referencing the original document.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        template = PromptTemplate(\n",
        "            input_variables=[\"document\", \"summary\"],\n",
        "            template=REFINE_SUMMARY_PROMPT\n",
        "        )\n",
        "        local_chain = LLMChain(llm=refine_chain.llm, prompt=template)\n",
        "        result = local_chain.run(document=document, summary=existing_summary)\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error refining summary: %s\", e)\n",
        "        return existing_summary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dockerfile and requirements.txt\n",
        "\n",
        "Below are the Dockerfile and requirements.txt to make the entire pipeline production-ready. Adjust them as needed for your environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile Dockerfile\n",
        "FROM python:3.9-slim\n",
        "\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
        "    build-essential \\\n",
        "    git \\\n",
        " && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "COPY . .\n",
        "\n",
        "EXPOSE 8000\n",
        "\n",
        "# Default command (CLI). Adjust as needed.\n",
        "CMD [\"python\", \"ai_ml/main.py\", \"document.txt\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile requirements.txt\n",
        "# Core ML libraries\n",
        "torch>=1.10.0\n",
        "transformers>=4.30.0\n",
        "langchain>=0.0.220\n",
        "\n",
        "# ONNX and runtime support\n",
        "onnx>=1.12.0\n",
        "onnxruntime>=1.14.0\n",
        "\n",
        "# Optional: Hugging Face Optimum for enhanced ONNX support\n",
        "optimum[onnxruntime]>=1.12.0\n",
        "\n",
        "# For environment variable management (optional)\n",
        "python-dotenv>=0.21.0\n",
        "\n",
        "# FastAPI and Uvicorn for server.py example\n",
        "fastapi>=0.95.0\n",
        "uvicorn>=0.21.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Optional: `server.py` (FastAPI)\n",
        "\n",
        "If you want to expose an HTTP API, here’s a simple FastAPI server. It has one endpoint `/analyze` which calls `analyze_document` from `backend.py`. You can extend it to handle other endpoints for the extended features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile server.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "from ai_ml.backend import analyze_document\n",
        "\n",
        "app = FastAPI(title=\"Document Analysis API\")\n",
        "\n",
        "class AnalysisRequest(BaseModel):\n",
        "    document: str\n",
        "    question: str = None\n",
        "    translate_lang: str = \"fr\"\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze(req: AnalysisRequest):\n",
        "    try:\n",
        "        results = analyze_document(\n",
        "            document=req.document,\n",
        "            question=req.question,\n",
        "            translate_lang=req.translate_lang\n",
        "        )\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"server:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Usage Instructions\n",
        "\n",
        "1. **Install Dependencies**\n",
        "   ```bash\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "2. **(Optional) Convert Models to ONNX**\n",
        "   ```bash\n",
        "   python convert_to_onnx.py\n",
        "   ```\n",
        "   Then set `USE_ONNX=true` in your environment if you want to use the ONNX versions.\n",
        "\n",
        "3. **Run the CLI**\n",
        "   ```bash\n",
        "   python ai_ml/main.py document.txt --question \"What is the main idea?\" --translate_lang de\n",
        "   ```\n",
        "\n",
        "4. **Run the FastAPI Server** (optional)\n",
        "   ```bash\n",
        "   uvicorn server:app --host 0.0.0.0 --port 8000 --reload\n",
        "   ```\n",
        "   Then send a POST request to `http://localhost:8000/analyze` with JSON body:\n",
        "   ```json\n",
        "   {\n",
        "       \"document\": \"Your document text...\",\n",
        "       \"question\": \"Some question...\",\n",
        "       \"translate_lang\": \"fr\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "5. **Docker Build and Run** (optional)\n",
        "   ```bash\n",
        "   docker build -t document-analysis:latest .\n",
        "   docker run -p 8000:8000 document-analysis:latest\n",
        "   ```\n",
        "\n",
        "6. **Extended Features**\n",
        "   - Key ideas, bullet summaries, rewriting, recommendations, summary refinement, etc., can be integrated by loading or creating a dedicated `LLMChain` (like we do for summarization). Then call the relevant function from `ai_ml/extended_features/`. Example:\n",
        "   ```python\n",
        "   from ai_ml.extended_features.key_ideas_extractor import generate_key_ideas\n",
        "   key_ideas = generate_key_ideas(document, my_key_ideas_chain)\n",
        "   print(key_ideas)\n",
        "   ```\n",
        "\n",
        "Enjoy your **complete** AI/ML pipeline with ONNX support, extended features, Dockerization, and more!"
      ]
    }
  ],
  "metadata": {
    "name": "Complete_AIML_Pipeline",
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
